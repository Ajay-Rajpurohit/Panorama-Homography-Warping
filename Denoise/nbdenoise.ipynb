{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import imageio\n",
    "import random\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from tensorflow.keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re \n",
    "from exceptions import *\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "MINIMUM_MATCH_POINTS = 15 #The minimum number of matched keypoints required to consider the matching process successfu\n",
    "CONFIDENCE_THRESH = 10 # confidence percentage threshold of match points used for homography computation\n",
    "\n",
    "\n",
    "\n",
    "def plot_interactive_keypoints(image, keypoints, title):\n",
    "    fig = go.Figure(data=[go.Scatter(x=[kp[0] for kp in keypoints], y=[kp[1] for kp in keypoints],\n",
    "                                     mode='markers', marker=dict(size=5, color='purple'))])\n",
    "    fig.update_layout(title=title, xaxis_title='X Coordinate', yaxis_title='Y Coordinate',\n",
    "                      width=800, height=600, autosize=False)\n",
    "    fig.update_xaxes(range=[0, image.shape[1]])\n",
    "    fig.update_yaxes(range=[0, image.shape[0]], autorange=\"reversed\")  # Image coordinates are reversed in y-axis\n",
    "    fig.show()\n",
    "\n",
    "def draw_keypoints(vis, keypoints, color):\n",
    "    for kp in keypoints:\n",
    "        x, y = kp.pt\n",
    "        cv2.circle(vis, (int(x), int(y)), 5, color, -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_matches(img_a_gray, img_b_gray, num_keypoints=1000, threshold=0.8):\n",
    "    '''Function to get matched keypoints from two images using ORB\n",
    "\n",
    "    Args:\n",
    "        img_a_gray (numpy array): of shape (H, W) representing grayscale image A\n",
    "        img_b_gray (numpy array): of shape (H, W) representing grayscale image B\n",
    "        num_keypoints (int): number of points to be matched (default=100)\n",
    "        threshold (float): can be used to filter strong matches only. \n",
    "        Lower the value, stronger the requirements and hence fewer matches.\n",
    "        \n",
    "        A lower threshold value indicates stronger matches because it requires the closest match \n",
    "        (first nearest neighbor) to be significantly closer than the second nearest neighbor.\n",
    "        This large disparity ensures that the closest \n",
    "        match is much more likely to be correct, reducing the chance of false matches.\n",
    "    Returns:\n",
    "        match_points_a (numpy array): of shape (n, 2) representing x,y pixel coordinates of image A keypoints\n",
    "        match_points_b (numpy array): of shape (n, 2) representing x,y pixel coordianted of matched keypoints in image B\n",
    "    '''\n",
    "    orb = cv2.ORB_create(nfeatures=num_keypoints)\n",
    "    # find keypoints and their descriptors\n",
    "    kp_a, desc_a = orb.detectAndCompute(img_a_gray, None)\n",
    "    kp_b, desc_b = orb.detectAndCompute(img_b_gray, None)\n",
    "    \n",
    "    #brute force matcher using the hamming distance, gets closest matches\n",
    "    dis_matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "    # specifically retrieves the k nearest neighbors for each descriptor\n",
    "    matches_list = dis_matcher.knnMatch(desc_a, desc_b, k=2) # get the two nearest matches for each keypoint in image A\n",
    "\n",
    "    # for each keypoint feature in image A, compare the distance of the two matched keypoints in image B\n",
    "    # retain only if distance is less than a threshold \n",
    "    good_matches_list = []\n",
    "    for match_1, match_2 in matches_list:\n",
    "        # if the distance of the closer match (match_1) is less than threshold * distance\n",
    "        #  of the farther match (match_2), match_1 is considered a good match.\n",
    "        if match_1.distance < threshold * match_2.distance:\n",
    "            good_matches_list.append(match_1)\n",
    "    \n",
    "    #filter good matching keypoints \n",
    "    good_kp_a = []\n",
    "    good_kp_b = []\n",
    "    \n",
    "    # For each match, retrieve the coordinate of the keypoint in image A (good_kp_a) using queryIdx and \n",
    "    # in image B (good_kp_b) using trainIdx, \n",
    "    # ensuring that these keypoints correspond to each other across the two images.\n",
    "    for match in good_matches_list:\n",
    "        # .pt is an attribute of a keypoint object returned by feature detection methods like ORB. The .pt attribute\n",
    "        # contains the (x, y) coordinates of the keypoint in the image. \n",
    "        good_kp_a.append(kp_a[match.queryIdx].pt) # keypoint in image A\n",
    "        good_kp_b.append(kp_b[match.trainIdx].pt) # matching keypoint in image B\n",
    "\n",
    "    img_kp_a = cv2.drawKeypoints(img_a_gray, kp_a, None, color=(0, 255, 0), flags=0)\n",
    "    img_kp_b = cv2.drawKeypoints(img_b_gray, kp_b, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img_kp_a, cmap='gray')\n",
    "    plt.title('Keypoints in Image A')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img_kp_b, cmap='gray')\n",
    "    plt.title('Keypoints in Image B')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization of keypoints and matches\n",
    "    img_matches = cv2.drawMatches(img_a_gray, kp_a, img_b_gray, kp_b, good_matches_list, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(img_matches)\n",
    "    plt.title('Key Points and Matches')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if len(good_kp_a) < MINIMUM_MATCH_POINTS:\n",
    "        raise NotEnoughMatchPointsError(len(good_kp_a), MINIMUM_MATCH_POINTS)\n",
    "    \n",
    "    return np.array(good_kp_a), np.array(good_kp_b)\n",
    "\n",
    "\n",
    "def calculate_homography(points_img_a, points_img_b):\n",
    "    '''Function to calculate the homography matrix from point corresspondences using Direct Linear Transformation\n",
    "        The resultant homography transforms points in image B into points in image A\n",
    "        Homography H = [h1 h2 h3; \n",
    "                        h4 h5 h6;\n",
    "                        h7 h8 h9]\n",
    "        u, v ---> point in image A\n",
    "        x, y ---> matched point in image B then,\n",
    "        with n point correspondences the DLT equation is:\n",
    "            A.h = 0\n",
    "        where A = [-x1 -y1 -1 0 0 0 u1*x1 u1*y1 u1;\n",
    "                   0 0 0 -x1 -y1 -1 v1*x1 v1*y1 v1;\n",
    "                   ...............................;\n",
    "                   ...............................;\n",
    "                   -xn -yn -1 0 0 0 un*xn un*yn un;\n",
    "                   0 0 0 -xn -yn -1 vn*xn vn*yn vn]\n",
    "        This equation is then solved using SVD\n",
    "        (At least 4 point correspondences are required to determine 8 unkwown parameters of homography matrix)\n",
    "    Args:\n",
    "        points_img_a (numpy array): of shape (n, 2) representing pixel coordinate points (u, v) in image A\n",
    "        points_img_b (numpy array): of shape (n, 2) representing pixel coordinates (x, y) in image B\n",
    "    \n",
    "    Returns:\n",
    "        h_mat: A (3, 3) numpy array of estimated homography\n",
    "    '''\n",
    "    # concatenate the two numpy points array to get 4 columns (u, v, x, y)\n",
    "    points_a_and_b = np.concatenate((points_img_a, points_img_b), axis=1)\n",
    "    A = []\n",
    "    # fill the A matrix by looping through each row of points_a_and_b containing u, v, x, y\n",
    "    # each row in the points_ab would fill two rows in the A matrix\n",
    "    for u, v, x, y in points_a_and_b:\n",
    "        A.append([-x, -y, -1, 0, 0, 0, u*x, u*y, u])\n",
    "        A.append([0, 0, 0, -x, -y, -1, v*x, v*y, v])\n",
    "    \n",
    "    A = np.array(A)\n",
    "    _, _, v_t = np.linalg.svd(A)\n",
    "\n",
    "    # soltion is the last column of v which means the last row of its transpose v_t\n",
    "    h_mat = v_t[-1, :].reshape(3,3)\n",
    "    return h_mat\n",
    "\n",
    "def transform_with_homography(h_mat, points_array):\n",
    "    \"\"\"Function to transform a set of points using the given homography matrix.\n",
    "        Points are normalized after transformation with the last column which represents the scale\n",
    "    \n",
    "    Args:\n",
    "        h_mat (numpy array): of shape (3, 3) representing the homography matrix\n",
    "        points_array (numpy array): of shape (n, 2) represting n set of x, y pixel coordinates that are\n",
    "            to be transformed\n",
    "    \"\"\"\n",
    "    # add column of ones so that matrix multiplication with homography matrix is possible\n",
    "    ones_col = np.ones((points_array.shape[0], 1))\n",
    "    # axis 1 means horizontally a new column of ones is added\n",
    "    points_array = np.concatenate((points_array, ones_col), axis=1)\n",
    "\n",
    "    # multiply the homography matrix to get the transformed points from points array\n",
    "    transformed_points = np.matmul(h_mat, points_array.T)\n",
    "    epsilon = 1e-7 # very small value to use it during normalization to avoid division by zero\n",
    "\n",
    "# Access the third row which contains the scale \n",
    "# factors (w) for each point, is accessed.\n",
    "# reshaped into a 2D array with one row to allow for element-wise division \n",
    "# Epsilon to prevent division by zero in case any scale factor is zero.\n",
    "# All coordinates  are divided by these adjusted scale factors,\n",
    "#  converting points from homogeneous coordinates (x, y, w) back to \n",
    "# Cartesian coordinates (x/w, y/w), effectively normalizing \n",
    "    transformed_points = transformed_points / (transformed_points[2,:].reshape(1,-1) + epsilon)\n",
    "    transformed_points = transformed_points[0:2,:].T\n",
    "    return transformed_points\n",
    "\n",
    "\n",
    "def compute_outliers(h_mat, points_img_a, points_img_b, threshold=3):\n",
    "    '''Function to compute the error in the Homography matrix using the matching points in\n",
    "        image A and image B\n",
    "    \n",
    "    Args:\n",
    "        h_mat (numpy array): of shape (3, 3) representing the homography that transforms points in image B to points in image A\n",
    "        points_img_a (numpy array): of shape (n, 2) representing pixel coordinate points (u, v) in image A\n",
    "        points_img_b (numpy array): of shape (n, 2) representing pixel coordinates (x, y) in image B\n",
    "        theshold (int): a number that represents the allowable euclidean distance (in pixels) between the transformed pixel coordinate from\n",
    "            the image B to the matched pixel coordinate in image A, to be conisdered outliers\n",
    "    \n",
    "    Returns:\n",
    "        error: a scalar float representing the error in the Homography matrix\n",
    "    '''\n",
    "    num_points = points_img_a.shape[0]\n",
    "    outliers_count = 0\n",
    "\n",
    "    # transform the match point in image B to image A using the homography\n",
    "    points_img_b_hat = transform_with_homography(h_mat, points_img_b)\n",
    "    \n",
    "    # let x, y be coordinate representation of points in image A\n",
    "    # let x_hat, y_hat be the coordinate representation of transformed points of image B with respect to image A\n",
    "    x = points_img_a[:, 0]\n",
    "    y = points_img_a[:, 1]\n",
    "    x_hat = points_img_b_hat[:, 0]\n",
    "    y_hat = points_img_b_hat[:, 1]\n",
    "    euclid_dis = np.sqrt(np.power((x_hat - x), 2) + np.power((y_hat - y), 2)).reshape(-1)\n",
    "    for dis in euclid_dis:\n",
    "        if dis > threshold:\n",
    "            outliers_count += 1\n",
    "    return outliers_count\n",
    "\n",
    "\n",
    "# def compute_homography_ransac(matches_a, matches_b):\n",
    "#     \"\"\"Function to estimate the best homography matrix using RANSAC on potentially matching\n",
    "#     points.\n",
    "    \n",
    "#     Args:\n",
    "#         matches_a (numpy array): of shape (n, 2) representing the coordinates\n",
    "#             of possibly matching points in image A\n",
    "#         matches_b (numpy array): of shape (n, 2) representing the coordinates\n",
    "#             of possibly matching points in image B\n",
    "\n",
    "#     Returns:\n",
    "#         best_h_mat: A numpy array of shape (3, 3) representing the best homography\n",
    "#             matrix that transforms points in image B to points in image A\n",
    "#     \"\"\"\n",
    "#     num_all_matches =  matches_a.shape[0]\n",
    "#     # RANSAC parameters\n",
    "#     SAMPLE_SIZE = 5 #number of point correspondances for estimation of Homgraphy\n",
    "#     SUCCESS_PROB = 0.995 #required probabilty of finding H with all samples being inliners \n",
    "#     min_iterations = int(np.log(1.0 - SUCCESS_PROB)/np.log(1 - 0.5**SAMPLE_SIZE))\n",
    "    \n",
    "#     # Let the initial error be large i.e consider all matched points as outliers\n",
    "#     lowest_outliers_count = num_all_matches\n",
    "#     best_h_mat = None\n",
    "#     best_i = 0 # just to know in which iteration the best h_mat was found\n",
    "\n",
    "#     for i in range(min_iterations):\n",
    "#         rand_ind = np.random.permutation(range(num_all_matches))[:SAMPLE_SIZE]\n",
    "#         h_mat = calculate_homography(matches_a[rand_ind], matches_b[rand_ind])\n",
    "#         outliers_count = compute_outliers(h_mat, matches_a, matches_b)\n",
    "#         if outliers_count < lowest_outliers_count:\n",
    "#             best_h_mat = h_mat\n",
    "#             lowest_outliers_count = outliers_count\n",
    "#             best_i = i\n",
    "#     best_confidence_obtained = int(100 - (100 * lowest_outliers_count / num_all_matches))\n",
    "#     if best_confidence_obtained < CONFIDENCE_THRESH:\n",
    "#         raise(exceptions.MatchesNotConfident(best_confidence_obtained))\n",
    "#     return best_h_mat\n",
    "\n",
    "def compute_homography_ransac(matches_a, matches_b):\n",
    "    \"\"\"Function to estimate the best homography matrix using RANSAC on potentially matching\n",
    "    points.\n",
    "    \n",
    "    Args:\n",
    "        matches_a (numpy array): of shape (n, 2) representing the coordinates\n",
    "            of possibly matching points in image A\n",
    "        matches_b (numpy array): of shape (n, 2) representing the coordinates\n",
    "            of possibly matching points in image B\n",
    "\n",
    "    Returns:\n",
    "        best_h_mat: A numpy array of shape (3, 3) representing the best homography\n",
    "            matrix that transforms points in image B to points in image A\n",
    "    \"\"\"\n",
    "\n",
    "    # def compute_homography_ransac(matches_a, m\n",
    "    print(\"Running modified compute_homography_ransac\")\n",
    "    # existing implementation\n",
    "\n",
    "    num_all_matches = matches_a.shape[0]\n",
    "    SAMPLE_SIZE = 5\n",
    "    SUCCESS_PROB = 0.995\n",
    "    # RANSAC algorithm's guidelines to calculate the likelihood of success over several iterations.\n",
    "    min_iterations = int(np.log(1.0 - SUCCESS_PROB) / np.log(1 - 0.5**SAMPLE_SIZE))\n",
    "\n",
    "    lowest_outliers_count = num_all_matches\n",
    "    best_h_mat = None\n",
    "\n",
    "    for i in range(min_iterations):\n",
    "        # select random set of points of sample size\n",
    "        rand_ind = np.random.permutation(range(num_all_matches))[:SAMPLE_SIZE]\n",
    "        # calculate homography matrix based on this\n",
    "        h_mat = calculate_homography(matches_a[rand_ind], matches_b[rand_ind])\n",
    "        # then given the matrix and the matches of both images, count the outliers\n",
    "        outliers_count = compute_outliers(h_mat, matches_a, matches_b)\n",
    "        # if it is lower make this the best homography matrix\n",
    "        if outliers_count < lowest_outliers_count:\n",
    "            best_h_mat = h_mat\n",
    "            lowest_outliers_count = outliers_count\n",
    "\n",
    "    # calculates the percentage of matches that are outliers.\n",
    "    # subtracting from 100 gives inliers percentage \n",
    "    best_confidence_obtained = int(100 - (100 * lowest_outliers_count / num_all_matches))\n",
    "    print(\"confidence obtained in ransac\", best_confidence_obtained)\n",
    "    if best_confidence_obtained < CONFIDENCE_THRESH:\n",
    "        raise(exceptions.MatchesNotConfident(best_confidence_obtained, CONFIDENCE_THRESH))\n",
    "    return best_h_mat\n",
    "\n",
    "def get_corners_as_array(img_height, img_width):\n",
    "    \"\"\"Function to extract the corner points of an image from its width and height \n",
    "        and arrange it in the form\n",
    "        of a numpy array.\n",
    "        \n",
    "        The 4 corners are arranged as follows:\n",
    "        corners = [top_left_x, top_left_y;\n",
    "                   top_right_x, top_right_y;\n",
    "                   bottom_right_x, bottom_right_y;\n",
    "                   bottom_left_x, bottom_left_y]\n",
    "\n",
    "    Args:\n",
    "        img_height (str): height of the image\n",
    "        img_width (str): width of the image\n",
    "    \n",
    "    Returns:\n",
    "        corner_points_array (numpy array): of shape (4,2) representing for corners with x,y pixel coordinates\n",
    "    \"\"\"\n",
    "    corners_array = np.array([[0, 0],\n",
    "                            [img_width - 1, 0],\n",
    "                            [img_width - 1, img_height - 1],\n",
    "                            [0, img_height - 1]])\n",
    "    return corners_array\n",
    "\n",
    "\n",
    "def get_crop_points_horz(img_a_h, transfmd_corners_img_b):\n",
    "    \"\"\"Function to find the pixel corners in the horizontally stitched images to crop and remove the\n",
    "        black space around.\n",
    "    \n",
    "    Args:   \n",
    "        img_a_h (int): the height of the pivot image that is image A\n",
    "        transfmd_corners_img_b (numpy array): of shape (n, 2) representing the transformed corners of image B\n",
    "            The corners need to be in the following sequence:\n",
    "            corners = [top_left_x, top_left_y;\n",
    "                   top_right_x, top_right_y;\n",
    "                   bottom_right_x, bottom_right_y;\n",
    "                   bottom_left_x, bottom_left_y]\n",
    "    Returns:\n",
    "        x_start (int): the x pixel-cordinate to start the crop on the stitched image\n",
    "        y_start (int): the x pixel-cordinate to start the crop on the stitched image\n",
    "        x_end (int): the x pixel-cordinate to end the crop on the stitched image\n",
    "        y_end (int): the y pixel-cordinate to end the crop on the stitched image\n",
    "    \"\"\"\n",
    "    # the four transformed corners of image B\n",
    "    top_lft_x_hat, top_lft_y_hat = transfmd_corners_img_b[0, :]\n",
    "    top_rht_x_hat, top_rht_y_hat = transfmd_corners_img_b[1, :]\n",
    "    btm_rht_x_hat, btm_rht_y_hat = transfmd_corners_img_b[2, :]\n",
    "    btm_lft_x_hat, btm_lft_y_hat = transfmd_corners_img_b[3, :]\n",
    "\n",
    "    # initialize the crop points\n",
    "    # since image A (on the left side) is used as pivot, x_start will always be zero\n",
    "    x_start, y_start, x_end, y_end = (0, None, None, None)\n",
    "\n",
    "    if (top_lft_y_hat > 0) and (top_lft_y_hat > top_rht_y_hat):\n",
    "        y_start = top_lft_y_hat\n",
    "    elif (top_rht_y_hat > 0) and (top_rht_y_hat > top_lft_y_hat):\n",
    "        y_start = top_rht_y_hat\n",
    "    else:\n",
    "        y_start = 0\n",
    "        \n",
    "    if (btm_lft_y_hat < img_a_h - 1) and (btm_lft_y_hat < btm_rht_y_hat):\n",
    "        y_end = btm_lft_y_hat\n",
    "    elif (btm_rht_y_hat < img_a_h - 1) and (btm_rht_y_hat < btm_lft_y_hat):\n",
    "        y_end = btm_rht_y_hat\n",
    "    else:\n",
    "        y_end = img_a_h - 1\n",
    "\n",
    "    if (top_rht_x_hat < btm_rht_x_hat):\n",
    "        x_end = top_rht_x_hat\n",
    "    else:\n",
    "        x_end = btm_rht_x_hat\n",
    "    \n",
    "    return int(x_start), int(y_start), int(x_end), int(y_end)\n",
    "\n",
    "\n",
    "def get_crop_points_vert(img_a_w, transfmd_corners_img_b):\n",
    "    \"\"\"Function to find the pixel corners in the vertically stitched images to crop and remove the\n",
    "        black space around.\n",
    "    \n",
    "    Args:\n",
    "        img_a_h (int): the width of the pivot image that is image A\n",
    "        transfmd_corners_img_b (numpy array): of shape (n, 2) representing the transformed corners of image B\n",
    "            The corners need to be in the following sequence:\n",
    "            corners = [top_left_x, top_left_y;\n",
    "                   top_right_x, top_right_y;\n",
    "                   bottom_right_x, bottom_right_y;\n",
    "                   bottom_left_x, bottom_left_y]\n",
    "    Returns:\n",
    "        x_start (int): the x pixel-cordinate to start the crop on the stitched image\n",
    "        y_start (int): the x pixel-cordinate to start the crop on the stitched image\n",
    "        x_end (int): the x pixel-cordinate to end the crop on the stitched image\n",
    "        y_end (int): the y pixel-cordinate to end the crop on the stitched image\n",
    "    \"\"\"\n",
    "    # the four transformed corners of image B\n",
    "    top_lft_x_hat, top_lft_y_hat = transfmd_corners_img_b[0, :]\n",
    "    top_rht_x_hat, top_rht_y_hat = transfmd_corners_img_b[1, :]\n",
    "    btm_rht_x_hat, btm_rht_y_hat = transfmd_corners_img_b[2, :]\n",
    "    btm_lft_x_hat, btm_lft_y_hat = transfmd_corners_img_b[3, :]\n",
    "\n",
    "    # initialize the crop points\n",
    "    # since image A (on the top) is used as pivot, y_start will always be zero\n",
    "    x_start, y_start, x_end, y_end = (None, 0, None, None)\n",
    "\n",
    "    if (top_lft_x_hat > 0) and (top_lft_x_hat > btm_lft_x_hat):\n",
    "        x_start = top_lft_x_hat\n",
    "    elif (btm_lft_x_hat > 0) and (btm_lft_x_hat > top_lft_x_hat):\n",
    "        x_start = btm_lft_x_hat\n",
    "    else:\n",
    "        x_start = 0\n",
    "        \n",
    "    if (top_rht_x_hat < img_a_w - 1) and (top_rht_x_hat < btm_rht_x_hat):\n",
    "        x_end = top_rht_x_hat\n",
    "    elif (btm_rht_x_hat < img_a_w - 1) and (btm_rht_x_hat < top_rht_x_hat):\n",
    "        x_end = btm_rht_x_hat\n",
    "    else:\n",
    "        x_end = img_a_w - 1\n",
    "\n",
    "    if (btm_lft_y_hat < btm_rht_y_hat):\n",
    "        y_end = btm_lft_y_hat\n",
    "    else:\n",
    "        y_end = btm_rht_y_hat\n",
    "    \n",
    "    return int(x_start), int(y_start), int(x_end), int(y_end)\n",
    "\n",
    "\n",
    "def get_crop_points(h_mat, img_a, img_b, stitch_direc):\n",
    "    \"\"\"Function to find the pixel corners to crop the stitched image such that the black space \n",
    "        in the stitched image is removed.\n",
    "        The black space could be because either image B is not of the same dimensions as image A\n",
    "        or image B is skewed after homographic transformation.\n",
    "        Example: \n",
    "                  (Horizontal stitching)\n",
    "                ____________                     _________________\n",
    "                |           |                    |                |\n",
    "                |           |__________          |                |\n",
    "                |           |         /          |       A        |\n",
    "                |     A     |   B    /           |________________|\n",
    "                |           |       /                |          | \n",
    "                |           |______/                 |    B     |\n",
    "                |___________|                        |          |\n",
    "                                                     |__________|  <-imagine slant bottom edge\n",
    "        \n",
    "        This function returns the corner points to obtain the maximum area inside A and B combined and making\n",
    "        sure the edges are straight (i.e horizontal and veritcal). \n",
    "\n",
    "    Args:\n",
    "        h_mat (numpy array): of shape (3, 3) representing the homography from image B to image A\n",
    "        img_a (numpy array): of shape (h, w, c) representing image A\n",
    "        img_b (numpy array): of shape (h, w, c) representing image B\n",
    "        stitch_direc (int): 0 when stitching vertically and 1 when stitching horizontally\n",
    "\n",
    "    Returns:\n",
    "        x_start (int): the x pixel-cordinate to start the crop on the stitched image\n",
    "        y_start (int): the x pixel-cordinate to start the crop on the stitched image\n",
    "        x_end (int): the x pixel-cordinate to end the crop on the stitched image\n",
    "        y_end (int): the y pixel-cordinate to end the crop on the stitched image          \n",
    "    \"\"\"\n",
    "\n",
    "    # extract height and width of the images\n",
    "    img_a_h, img_a_w, _ = img_a.shape\n",
    "    img_b_h, img_b_w, _ = img_b.shape\n",
    "\n",
    "    # corner points of image b\n",
    "    orig_corners_img_b = get_corners_as_array(img_b_h, img_b_w)\n",
    "\n",
    "    # applies homography and returns transformed corners of the image b \n",
    "    transfmd_corners_img_b = transform_with_homography(h_mat, orig_corners_img_b)\n",
    "\n",
    "    # if stitch_direc == 1:\n",
    "    #     x_start, y_start, x_end, y_end = get_crop_points_horz(img_a_w, transfmd_corners_img_b)\n",
    "    # initialize the crop points\n",
    "    x_start = None\n",
    "    x_end = None\n",
    "    y_start = None\n",
    "    y_end = None\n",
    "\n",
    "    if stitch_direc == 1: # 1 is horizontal\n",
    "        x_start, y_start, x_end, y_end = get_crop_points_horz(img_a_h, transfmd_corners_img_b)\n",
    "    else: # when stitching images in the vertical direction\n",
    "        x_start, y_start, x_end, y_end = get_crop_points_vert(img_a_w, transfmd_corners_img_b)\n",
    "    return x_start, y_start, x_end, y_end\n",
    "\n",
    "\n",
    "def stitch_image_pair(img_a, img_b, stitch_direc):\n",
    "    print(\"inside stich image pair function\")\n",
    "\n",
    "    \"\"\"Function to stitch image B to image A in the mentioned direction\n",
    "\n",
    "    Args:\n",
    "        img_a (numpy array): of shape (H, W, C) with opencv representation of image A (i.e C: B,G,R)\n",
    "        img_b (numpy array): of shape (H, W, C) with opencv representation of image B (i.e C: B,G,R)\n",
    "        stitch_direc (int): 0 for vertical and 1 for horizontal stitching\n",
    "\n",
    "    Returns:\n",
    "        stitched_image (numpy array): stitched image with maximum content of image A and image B after cropping\n",
    "            to remove the black space \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"inside stich image pair function\")\n",
    "    img_a_gray = cv2.cvtColor(img_a, cv2.COLOR_BGR2GRAY)\n",
    "    img_b_gray = cv2.cvtColor(img_b, cv2.COLOR_BGR2GRAY)\n",
    "    matches_a, matches_b = get_matches(img_a_gray, img_b_gray, num_keypoints=1000, threshold=0.8)\n",
    "\n",
    "    print(\"matching done\")\n",
    "\n",
    "    h_mat = compute_homography_ransac(matches_a, matches_b)\n",
    "\n",
    "    print(\"homography done\")\n",
    "\n",
    "    # on image b: img_b the transformation is done\n",
    "    # h_matrix defines the necessary transformation required to align b with image a\n",
    "\n",
    "    if stitch_direc == 0:\n",
    "        # width same as image a, height is the combined image height of a and b    \n",
    "        canvas = cv2.warpPerspective(img_b, h_mat, (img_a.shape[1], img_a.shape[0] + img_b.shape[0]))\n",
    "        # now the canvas has the warped image b\n",
    "        # now overlay image a in the portion of canvas where it is required\n",
    "        # aligning with top edge\n",
    "        canvas[0:img_a.shape[0], :, :] = img_a[:, :, :]\n",
    "        # get the points of canvas where image overlap properly so, to exclude\n",
    "        # or crop the remaining parts of image\n",
    "        x_start, y_start, x_end, y_end = get_crop_points(h_mat, img_a, img_b, 0)\n",
    "    else:\n",
    "        canvas = cv2.warpPerspective(img_b, h_mat, (img_a.shape[1] + img_b.shape[1], img_a.shape[0]))\n",
    "        canvas[:, 0:img_a.shape[1], :] = img_a[:, :, :]\n",
    "        x_start, y_start, x_end, y_end = get_crop_points(h_mat, img_a, img_b, 1)\n",
    "    \n",
    "    stitched_img = canvas[y_start:y_end,x_start:x_end,:]\n",
    "    return stitched_img\n",
    "\n",
    "\n",
    "def check_imgfile_validity(folder, filenames):\n",
    "    \"\"\"Function to check if the files in the given path are valid image files.\n",
    "    \n",
    "    Args:\n",
    "        folder (str): path containing the image files\n",
    "        filenames (list): a list of image filenames\n",
    "\n",
    "    Returns:\n",
    "        valid_files (bool): True if all the files are valid image files else False\n",
    "        msg (str): Message that has to be displayed as error\n",
    "    \"\"\"\n",
    "    for file in filenames:\n",
    "        full_file_path = os.path.join(folder, file)\n",
    "        regex = \"([^\\\\s]+(\\\\.(?i:(jpe?g|png)))$)\"\n",
    "        p = re.compile(regex)\n",
    "\n",
    "        if not os.path.isfile(full_file_path):\n",
    "            return False, \"File not found: \" + full_file_path\n",
    "        if not (re.search(p, file)):\n",
    "            return False, \"Invalid image file: \" + file\n",
    "    return True, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from exceptions import *\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def stitch_images(image_folder, image_filenames, stitch_direction):\n",
    "    \"\"\"Function to stitch a sequence of input images.\n",
    "        Images can be stitched horizontally or vertically.\n",
    "        For horizontal stitching the images have to be passed from left to right order in the scene.\n",
    "        For vertical stitching the images have to be passed from top to bottom order in the scene.\n",
    "    \n",
    "    Args:\n",
    "        image_folder (str): path of the directory containing the images\n",
    "        image_filenames (list): a list of image file names in the order of stitching\n",
    "        stitch_direction (int): 1 for horizontal stitching, 0 for vertical stitching\n",
    "    \n",
    "    Returns:\n",
    "        stitched_image (numpy array): of shape (H, W, 3) representing the stitched image\n",
    "    \"\"\"\n",
    "   \n",
    "    num_images = len(image_filenames)\n",
    "    \n",
    "    #if number of images is less than 2 raise error\n",
    "    if num_images < 2:\n",
    "        raise(exceptions.InsufficientImagesError(num_images))\n",
    "    \n",
    "    # check for files validity\n",
    "    valid_files, file_error_msg = check_imgfile_validity(image_folder, image_filenames)\n",
    "    if not valid_files:\n",
    "        raise(exceptions.InvalidImageFilesError(file_error_msg))\n",
    "    \n",
    "    print(\"valid files done\")\n",
    "    #takes first image as the pivot image\n",
    "    # In panorama stitching, the pivot image is the central reference \n",
    "    # image around which other images\n",
    "    # are aligned and stitched to create a seamless panoramic view.\n",
    "    pivot_img_path = os.path.join(image_folder, image_filenames[0])\n",
    "    pivot_img = cv2.imread(pivot_img_path)\n",
    "    print(\"read pivot image\")\n",
    "\n",
    "    # Loop through all images starting from the second image in the list\n",
    "    for i in range(1, num_images, 1):\n",
    "            # Construct the full path for each image and read it.\n",
    "        join_img_path = os.path.join(image_folder, image_filenames[i])\n",
    "        join_img = cv2.imread(join_img_path)\n",
    "        try:\n",
    "            print(\"About to start stitching pairs\")\n",
    "            print(\"i is \", i)\n",
    "            #stitch this image with pivot image\n",
    "            pivot_img = stitch_image_pair(pivot_img, join_img, stitch_direc=stitch_direction)\n",
    "            print(\"Finished stitching pairs\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "    return pivot_img\n",
    "\n",
    "def stitch_images_and_save(image_folder, image_filenames, stitch_direction, output_folder=None):\n",
    "    \"\"\"Function to stitch and save the resultant image.\n",
    "        Images can be stitched horizontally or vertically.\n",
    "        For horizontal stitching the images have to be passed from left to right order in the scene.\n",
    "        For vertical stitching the images have to be passed from top to bottom order in the scene.\n",
    "    \n",
    "    Args:\n",
    "        image_folder (str): path of the directory containing the images\n",
    "        image_filenames (list): a list of image file names in the order of stitching\n",
    "        stitch_direction (int): 1 for horizontal stitching, 0 for vertical stitching\n",
    "        output_folder (str): the directory to save the stitched image (default is None, which creates a directory named \"output\" to save)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = \"stitched_image_\" + timestr + \".jpg\"\n",
    "    stitched_img = stitch_images(image_folder, image_filenames, stitch_direction)\n",
    "    if output_folder is None:\n",
    "        if not os.path.isdir(\"output\"):\n",
    "            os.makedirs(\"output/\")\n",
    "        output_folder = \"output\"\n",
    "    full_save_path = os.path.join(output_folder, filename)\n",
    "    _ = cv2.imwrite(full_save_path, stitched_img)\n",
    "    print(\"The stitched image is saved at: \" + full_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hewllo World \n",
      "valid files done\n",
      "read pivot image\n",
      "About to start stitching pairs\n",
      "i is  1\n",
      "inside stich image pair function\n",
      "inside stich image pair function\n",
      "matching done\n",
      "Running modified compute_homography_ransac\n",
      "Heloooooo 74\n",
      "homography done\n",
      "Finished stitching pairs\n",
      "The stitched image is saved at: output/stitched_image_20240507_125805.jpg\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"img_folder\"\n",
    "# image_filenames=[\"scene1_a.jpg\", \"scene1_b.jpg\", \"scene1_c.jpg\"]\n",
    "# image_filenames=[\"scene2_a.jpg\", \"scene2_b.jpg\", \"scene2_c.jpg\"]\n",
    "image_filenames=[\"image1.jpg\", \"image2.jpg\"]\n",
    "# image_filenames=[\"badal1.jpeg\", \"badal2.jpeg\"]\n",
    "# image_filenames=[\"room1.jpeg\", \"room2.jpeg\", \"room3.jpeg\"]\n",
    "# image_filenames=[\"r1.jpeg\", \"r2.jpeg\"]\n",
    "\n",
    "print(\"Hewllo World \")\n",
    "stitch_direction=1\n",
    "#output_folder=\"./img_output\"\n",
    "\n",
    "#stitch_images(image_folder, image_filenames, stitch_direction)\n",
    "stitch_images_and_save(image_folder, image_filenames, stitch_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
